{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**PŔACTICA 02**\n",
        "\n",
        "Vargas Bravo Paola"
      ],
      "metadata": {
        "id": "QSjN8grFUd0I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reallizar al menos 5 entrenamientos y luego informa de los promedios de acurracy,precision, recall  y F1-score , tanto para la clase 0 como la clase , obtenidos de tus entrenamientos para cada una de las siguientes situaciones\n",
        "\n",
        "1.- epochs =4, batch size =2000, validation split =0.9\n",
        "\n",
        "2.- epochs =20, batch size =2000, validation split =0.9\n",
        "\n",
        "3.- epochs =4, batch size =2, validation split =0.9\n",
        "\n",
        "4.- epochs =4, batch size =2000, validation split =0.01"
      ],
      "metadata": {
        "id": "HtAtffMPUZMn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svrxwSaTUJBF",
        "outputId": "4e3b8493-6e67-4dce-b23f-5c430643a0c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entrenamiento 1 (epochs=4, batch_size=2000, validation_split=0.9):\n",
            "56/56 [==============================] - 0s 2ms/step\n",
            "56/56 [==============================] - 0s 1ms/step\n",
            "56/56 [==============================] - 0s 1ms/step\n",
            "56/56 [==============================] - 0s 2ms/step\n",
            "56/56 [==============================] - 0s 2ms/step\n",
            "Accuracy: 0.5091\n",
            "Precision (Clase 1): 0.5187\n",
            "Recall (Clase 1): 0.6013\n",
            "F1-score (Clase 1): 0.5224\n",
            "Precision (Clase 0): 0.5001\n",
            "Recall (Clase 0): 0.4168\n",
            "F1-score (Clase 0): 0.4044\n",
            "\n",
            "Entrenamiento 2 (epochs=20, batch_size=2000, validation_split=0.9):\n",
            "56/56 [==============================] - 0s 2ms/step\n",
            "56/56 [==============================] - 0s 1ms/step\n",
            "56/56 [==============================] - 0s 1ms/step\n",
            "56/56 [==============================] - 0s 2ms/step\n",
            "56/56 [==============================] - 0s 2ms/step\n",
            "Accuracy: 0.5539\n",
            "Precision (Clase 1): 0.5193\n",
            "Recall (Clase 1): 0.4711\n",
            "F1-score (Clase 1): 0.4440\n",
            "Precision (Clase 0): 0.5626\n",
            "Recall (Clase 0): 0.6368\n",
            "F1-score (Clase 0): 0.5608\n",
            "\n",
            "Entrenamiento 3 (epochs=4, batch_size=2, validation_split=0.9):\n",
            "56/56 [==============================] - 0s 2ms/step\n",
            "56/56 [==============================] - 0s 2ms/step\n",
            "56/56 [==============================] - 0s 1ms/step\n",
            "56/56 [==============================] - 0s 2ms/step\n",
            "56/56 [==============================] - 0s 2ms/step\n",
            "Accuracy: 0.5854\n",
            "Precision (Clase 1): 0.7348\n",
            "Recall (Clase 1): 0.4993\n",
            "F1-score (Clase 1): 0.4846\n",
            "Precision (Clase 0): 0.5321\n",
            "Recall (Clase 0): 0.6715\n",
            "F1-score (Clase 0): 0.5499\n",
            "\n",
            "Entrenamiento 4 (epochs=4, batch_size=2000, validation_split=0.01):\n",
            "56/56 [==============================] - 2s 2ms/step\n",
            "56/56 [==============================] - 0s 2ms/step\n",
            "56/56 [==============================] - 0s 3ms/step\n",
            "56/56 [==============================] - 0s 2ms/step\n",
            "56/56 [==============================] - 0s 2ms/step\n",
            "Accuracy: 0.6733\n",
            "Precision (Clase 1): 0.6682\n",
            "Recall (Clase 1): 0.7141\n",
            "F1-score (Clase 1): 0.6794\n",
            "Precision (Clase 0): 0.7073\n",
            "Recall (Clase 0): 0.6325\n",
            "F1-score (Clase 0): 0.6565\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense\n",
        "from keras.metrics import Precision, Recall, BinaryAccuracy, FalseNegatives, FalsePositives\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "def train_and_evaluate(epochs, batch_size, validation_split):\n",
        "    # Cargar los datos\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "    # Se obtiene una muestra con igual número de: cincos vs no cincos\n",
        "    # para el conjunto de entrenamiento y el de test\n",
        "    idx_y_train_5 = np.where(y_train == 5)[0]\n",
        "    idx_y_train_n5 = np.where(y_train != 5)[0]\n",
        "    idx_y_train_n5 = np.random.choice(idx_y_train_n5, idx_y_train_5.shape[0], replace=False)\n",
        "\n",
        "    idx_y_train = np.concatenate([idx_y_train_5, idx_y_train_n5])\n",
        "    np.random.shuffle(idx_y_train)\n",
        "\n",
        "    x_train = x_train[idx_y_train]\n",
        "    y_train = y_train[idx_y_train]\n",
        "\n",
        "    idx_y_test_5 = np.where(y_test == 5)[0]\n",
        "    idx_y_test_n5 = np.where(y_test != 5)[0]\n",
        "    idx_y_test_n5 = np.random.choice(idx_y_test_n5, idx_y_test_5.shape[0], replace=False)\n",
        "\n",
        "    idx_y_test = np.concatenate([idx_y_test_5, idx_y_test_n5])\n",
        "    np.random.shuffle(idx_y_test)\n",
        "\n",
        "    x_test = x_test[idx_y_test]\n",
        "    y_test = y_test[idx_y_test]\n",
        "\n",
        "    y_train_binary = (y_train == 5).astype(int)\n",
        "    y_test_binary = (y_test == 5).astype(int)\n",
        "\n",
        "    # Crear y compilar el modelo\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=(28, 28)),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', metrics=[BinaryAccuracy(), Precision(), Recall(), FalseNegatives(), FalsePositives()])\n",
        "\n",
        "    # Entrenamiento\n",
        "    model.fit(x_train, y_train_binary, epochs=epochs, batch_size=batch_size, validation_split=validation_split, verbose=0)\n",
        "\n",
        "    # Evaluación\n",
        "    y_pred = (model.predict(x_test) >= 0.5).astype(int)[:,0]\n",
        "\n",
        "    accuracy = model.evaluate(x_test, y_test_binary, verbose=0)[1]\n",
        "    precision_1 = precision_score(y_test_binary, y_pred)\n",
        "    recall_1 = recall_score(y_test_binary, y_pred)\n",
        "    f1_s_1 = f1_score(y_test_binary, y_pred)\n",
        "\n",
        "    precision_0 = precision_score(y_test_binary, y_pred, pos_label=0)\n",
        "    recall_0 = recall_score(y_test_binary, y_pred, pos_label=0)\n",
        "    f1_s_0 = f1_score(y_test_binary, y_pred, pos_label=0)\n",
        "\n",
        "    return accuracy, precision_1, recall_1, f1_s_1, precision_0, recall_0, f1_s_0\n",
        "\n",
        "# Definir los parámetros de entrenamiento para cada situación\n",
        "situations = [\n",
        "    {\"epochs\": 4, \"batch_size\": 2000, \"validation_split\": 0.9},\n",
        "    {\"epochs\": 20, \"batch_size\": 2000, \"validation_split\": 0.9},\n",
        "    {\"epochs\": 4, \"batch_size\": 2, \"validation_split\": 0.9},\n",
        "    {\"epochs\": 4, \"batch_size\": 2000, \"validation_split\": 0.01}\n",
        "]\n",
        "\n",
        "# Realizar los entrenamientos y calcular los promedios de las métricas\n",
        "for i, situation in enumerate(situations):\n",
        "    print(f\"Entrenamiento {i+1} (epochs={situation['epochs']}, batch_size={situation['batch_size']}, validation_split={situation['validation_split']}):\")\n",
        "    accuracy_list, precision_1_list, recall_1_list, f1_s_1_list, precision_0_list, recall_0_list, f1_s_0_list = [], [], [], [], [], [], []\n",
        "    for _ in range(5):  # Realizar 5 entrenamientos para cada situación\n",
        "        accuracy, precision_1, recall_1, f1_s_1, precision_0, recall_0, f1_s_0 = train_and_evaluate(situation[\"epochs\"], situation[\"batch_size\"], situation[\"validation_split\"])\n",
        "        accuracy_list.append(accuracy)\n",
        "        precision_1_list.append(precision_1)\n",
        "        recall_1_list.append(recall_1)\n",
        "        f1_s_1_list.append(f1_s_1)\n",
        "        precision_0_list.append(precision_0)\n",
        "        recall_0_list.append(recall_0)\n",
        "        f1_s_0_list.append(f1_s_0)\n",
        "\n",
        "    # Calcular promedios\n",
        "    accuracy_avg = np.mean(accuracy_list)\n",
        "    precision_1_avg = np.mean(precision_1_list)\n",
        "    recall_1_avg = np.mean(recall_1_list)\n",
        "    f1_s_1_avg = np.mean(f1_s_1_list)\n",
        "    precision_0_avg = np.mean(precision_0_list)\n",
        "    recall_0_avg = np.mean(recall_0_list)\n",
        "    f1_s_0_avg = np.mean(f1_s_0_list)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy_avg:.4f}\")\n",
        "    print(f\"Precision (Clase 1): {precision_1_avg:.4f}\")\n",
        "    print(f\"Recall (Clase 1): {recall_1_avg:.4f}\")\n",
        "    print(f\"F1-score (Clase 1): {f1_s_1_avg:.4f}\")\n",
        "    print(f\"Precision (Clase 0): {precision_0_avg:.4f}\")\n",
        "    print(f\"Recall (Clase 0): {recall_0_avg:.4f}\")\n",
        "    print(f\"F1-score (Clase 0): {f1_s_0_avg:.4f}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Busca una configuración de estos hiperparámetros que optimice las métricas, tu notebook debe tener un entrenamiento con esta configuración."
      ],
      "metadata": {
        "id": "OACMKEDHVfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense\n",
        "from keras.metrics import Precision, Recall, BinaryAccuracy, FalseNegatives, FalsePositives\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Cargar los datos\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Se obtiene una muestra con igual número de: cincos vs no cincos\n",
        "# para el conjunto de entrenamiento y el de test\n",
        "idx_y_train_5 = np.where(y_train == 5)[0]\n",
        "idx_y_train_n5 = np.where(y_train != 5)[0]\n",
        "idx_y_train_n5 = np.random.choice(idx_y_train_n5, idx_y_train_5.shape[0], replace=False)\n",
        "\n",
        "idx_y_train = np.concatenate([idx_y_train_5, idx_y_train_n5])\n",
        "np.random.shuffle(idx_y_train)\n",
        "\n",
        "x_train = x_train[idx_y_train]\n",
        "y_train = y_train[idx_y_train]\n",
        "\n",
        "idx_y_test_5 = np.where(y_test == 5)[0]\n",
        "idx_y_test_n5 = np.where(y_test != 5)[0]\n",
        "idx_y_test_n5 = np.random.choice(idx_y_test_n5, idx_y_test_5.shape[0], replace=False)\n",
        "\n",
        "idx_y_test = np.concatenate([idx_y_test_5, idx_y_test_n5])\n",
        "np.random.shuffle(idx_y_test)\n",
        "\n",
        "x_test = x_test[idx_y_test]\n",
        "y_test = y_test[idx_y_test]\n",
        "\n",
        "y_train_binary = (y_train == 5).astype(int)\n",
        "y_test_binary = (y_test == 5).astype(int)\n",
        "\n",
        "# Crear y compilar el modelo\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy', metrics=[BinaryAccuracy(), Precision(), Recall(), FalseNegatives(), FalsePositives()])\n",
        "\n",
        "# Entrenamiento\n",
        "model.fit(x_train, y_train_binary, epochs=20, batch_size=32, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Evaluación\n",
        "y_pred = (model.predict(x_test) >= 0.5).astype(int)[:,0]\n",
        "\n",
        "accuracy = model.evaluate(x_test, y_test_binary, verbose=0)[1]\n",
        "precision_1 = precision_score(y_test_binary, y_pred)\n",
        "recall_1 = recall_score(y_test_binary, y_pred)\n",
        "f1_s_1 = f1_score(y_test_binary, y_pred)\n",
        "\n",
        "precision_0 = precision_score(y_test_binary, y_pred, pos_label=0)\n",
        "recall_0 = recall_score(y_test_binary, y_pred, pos_label=0)\n",
        "f1_s_0 = f1_score(y_test_binary, y_pred, pos_label=0)\n",
        "\n",
        "print(f\"Entrenamiento {1} (epochs={20}, batch_size={32}, validation_split={0.2}):\")\n",
        "print(\"Metricas:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision (Class 1): {precision_1:.4f}\")\n",
        "print(f\"Recall (Class 1): {recall_1:.4f}\")\n",
        "print(f\"F1-score (Class 1): {f1_s_1:.4f}\")\n",
        "print(f\"Precision (Class 0): {precision_0:.4f}\")\n",
        "print(f\"Recall (Class 0): {recall_0:.4f}\")\n",
        "print(f\"F1-score (Class 0): {f1_s_0:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzW1bGWlUar6",
        "outputId": "1701e7c9-87fc-49e0-9075-04133760b0f8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "272/272 [==============================] - 2s 4ms/step - loss: 6.2148 - binary_accuracy: 0.5923 - precision_98: 0.5607 - recall_98: 0.8529 - false_negatives_98: 638.0000 - false_positives_98: 2898.0000 - val_loss: 5.0195 - val_binary_accuracy: 0.6727 - val_precision_98: 0.6670 - val_recall_98: 0.6891 - val_false_negatives_98: 337.0000 - val_false_positives_98: 373.0000\n",
            "Epoch 2/20\n",
            "272/272 [==============================] - 1s 3ms/step - loss: 4.7435 - binary_accuracy: 0.6903 - precision_98: 0.7327 - recall_98: 0.5993 - false_negatives_98: 1738.0000 - false_positives_98: 948.0000 - val_loss: 4.0212 - val_binary_accuracy: 0.7377 - val_precision_98: 0.7572 - val_recall_98: 0.6993 - val_false_negatives_98: 326.0000 - val_false_positives_98: 243.0000\n",
            "Epoch 3/20\n",
            "272/272 [==============================] - 1s 3ms/step - loss: 3.5181 - binary_accuracy: 0.7697 - precision_98: 0.8026 - recall_98: 0.7155 - false_negatives_98: 1234.0000 - false_positives_98: 763.0000 - val_loss: 2.9274 - val_binary_accuracy: 0.8077 - val_precision_98: 0.7923 - val_recall_98: 0.8339 - val_false_negatives_98: 180.0000 - val_false_positives_98: 237.0000\n",
            "Epoch 4/20\n",
            "272/272 [==============================] - 1s 3ms/step - loss: 2.9210 - binary_accuracy: 0.8094 - precision_98: 0.8243 - recall_98: 0.7865 - false_negatives_98: 926.0000 - false_positives_98: 727.0000 - val_loss: 2.4508 - val_binary_accuracy: 0.8400 - val_precision_98: 0.8674 - val_recall_98: 0.8026 - val_false_negatives_98: 214.0000 - val_false_positives_98: 133.0000\n",
            "Epoch 5/20\n",
            "272/272 [==============================] - 1s 3ms/step - loss: 2.6334 - binary_accuracy: 0.8276 - precision_98: 0.8668 - recall_98: 0.7743 - false_negatives_98: 979.0000 - false_positives_98: 516.0000 - val_loss: 2.6208 - val_binary_accuracy: 0.8290 - val_precision_98: 0.9219 - val_recall_98: 0.7186 - val_false_negatives_98: 305.0000 - val_false_positives_98: 66.0000\n",
            "Epoch 6/20\n",
            "272/272 [==============================] - 1s 3ms/step - loss: 2.6091 - binary_accuracy: 0.8291 - precision_98: 0.8148 - recall_98: 0.8520 - false_negatives_98: 642.0000 - false_positives_98: 840.0000 - val_loss: 3.1122 - val_binary_accuracy: 0.7948 - val_precision_98: 0.7205 - val_recall_98: 0.9631 - val_false_negatives_98: 40.0000 - val_false_positives_98: 405.0000\n",
            "Epoch 7/20\n",
            "272/272 [==============================] - 1s 3ms/step - loss: 2.2817 - binary_accuracy: 0.8503 - precision_98: 0.8484 - recall_98: 0.8531 - false_negatives_98: 637.0000 - false_positives_98: 661.0000 - val_loss: 2.3112 - val_binary_accuracy: 0.8492 - val_precision_98: 0.8955 - val_recall_98: 0.7906 - val_false_negatives_98: 227.0000 - val_false_positives_98: 100.0000\n",
            "Epoch 8/20\n",
            "272/272 [==============================] - 1s 3ms/step - loss: 2.1700 - binary_accuracy: 0.8576 - precision_98: 0.8674 - recall_98: 0.8444 - false_negatives_98: 675.0000 - false_positives_98: 560.0000 - val_loss: 2.2346 - val_binary_accuracy: 0.8538 - val_precision_98: 0.8051 - val_recall_98: 0.9336 - val_false_negatives_98: 72.0000 - val_false_positives_98: 245.0000\n",
            "Epoch 9/20\n",
            "272/272 [==============================] - 1s 4ms/step - loss: 2.1869 - binary_accuracy: 0.8567 - precision_98: 0.8314 - recall_98: 0.8949 - false_negatives_98: 456.0000 - false_positives_98: 787.0000 - val_loss: 2.2668 - val_binary_accuracy: 0.8515 - val_precision_98: 0.8164 - val_recall_98: 0.9068 - val_false_negatives_98: 101.0000 - val_false_positives_98: 221.0000\n",
            "Epoch 10/20\n",
            "272/272 [==============================] - 1s 5ms/step - loss: 2.5019 - binary_accuracy: 0.8366 - precision_98: 0.8974 - recall_98: 0.7602 - false_negatives_98: 1040.0000 - false_positives_98: 377.0000 - val_loss: 2.1782 - val_binary_accuracy: 0.8566 - val_precision_98: 0.8436 - val_recall_98: 0.8755 - val_false_negatives_98: 135.0000 - val_false_positives_98: 176.0000\n",
            "Epoch 11/20\n",
            "272/272 [==============================] - 1s 4ms/step - loss: 2.1313 - binary_accuracy: 0.8600 - precision_98: 0.8614 - recall_98: 0.8582 - false_negatives_98: 615.0000 - false_positives_98: 599.0000 - val_loss: 2.1453 - val_binary_accuracy: 0.8585 - val_precision_98: 0.9094 - val_recall_98: 0.7961 - val_false_negatives_98: 221.0000 - val_false_positives_98: 86.0000\n",
            "Epoch 12/20\n",
            "272/272 [==============================] - 1s 3ms/step - loss: 2.1267 - binary_accuracy: 0.8604 - precision_98: 0.8808 - recall_98: 0.8335 - false_negatives_98: 722.0000 - false_positives_98: 489.0000 - val_loss: 2.0348 - val_binary_accuracy: 0.8658 - val_precision_98: 0.8386 - val_recall_98: 0.9059 - val_false_negatives_98: 102.0000 - val_false_positives_98: 189.0000\n",
            "Epoch 13/20\n",
            "272/272 [==============================] - 1s 3ms/step - loss: 2.3513 - binary_accuracy: 0.8453 - precision_98: 0.8039 - recall_98: 0.9133 - false_negatives_98: 376.0000 - false_positives_98: 966.0000 - val_loss: 2.0808 - val_binary_accuracy: 0.8640 - val_precision_98: 0.9311 - val_recall_98: 0.7860 - val_false_negatives_98: 232.0000 - val_false_positives_98: 63.0000\n",
            "Epoch 14/20\n",
            "272/272 [==============================] - 1s 3ms/step - loss: 2.0798 - binary_accuracy: 0.8644 - precision_98: 0.8925 - recall_98: 0.8287 - false_negatives_98: 743.0000 - false_positives_98: 433.0000 - val_loss: 2.1658 - val_binary_accuracy: 0.8589 - val_precision_98: 0.9351 - val_recall_98: 0.7712 - val_false_negatives_98: 248.0000 - val_false_positives_98: 58.0000\n",
            "Epoch 15/20\n",
            "272/272 [==============================] - 1s 3ms/step - loss: 1.8763 - binary_accuracy: 0.8771 - precision_98: 0.8835 - recall_98: 0.8688 - false_negatives_98: 569.0000 - false_positives_98: 497.0000 - val_loss: 1.8656 - val_binary_accuracy: 0.8778 - val_precision_98: 0.9226 - val_recall_98: 0.8247 - val_false_negatives_98: 190.0000 - val_false_positives_98: 75.0000\n",
            "Epoch 16/20\n",
            "272/272 [==============================] - 1s 3ms/step - loss: 2.1579 - binary_accuracy: 0.8581 - precision_98: 0.8144 - recall_98: 0.9276 - false_negatives_98: 314.0000 - false_positives_98: 917.0000 - val_loss: 2.2263 - val_binary_accuracy: 0.8543 - val_precision_98: 0.7949 - val_recall_98: 0.9548 - val_false_negatives_98: 49.0000 - val_false_positives_98: 267.0000\n",
            "Epoch 17/20\n",
            "272/272 [==============================] - 1s 3ms/step - loss: 2.1514 - binary_accuracy: 0.8590 - precision_98: 0.8224 - recall_98: 0.9158 - false_negatives_98: 365.0000 - false_positives_98: 858.0000 - val_loss: 1.6975 - val_binary_accuracy: 0.8880 - val_precision_98: 0.8758 - val_recall_98: 0.9041 - val_false_negatives_98: 104.0000 - val_false_positives_98: 139.0000\n",
            "Epoch 18/20\n",
            "272/272 [==============================] - 1s 3ms/step - loss: 1.7985 - binary_accuracy: 0.8822 - precision_98: 0.8596 - recall_98: 0.9135 - false_negatives_98: 375.0000 - false_positives_98: 647.0000 - val_loss: 1.8732 - val_binary_accuracy: 0.8764 - val_precision_98: 0.8377 - val_recall_98: 0.9336 - val_false_negatives_98: 72.0000 - val_false_positives_98: 196.0000\n",
            "Epoch 19/20\n",
            "272/272 [==============================] - 1s 3ms/step - loss: 1.7224 - binary_accuracy: 0.8874 - precision_98: 0.8822 - recall_98: 0.8942 - false_negatives_98: 459.0000 - false_positives_98: 518.0000 - val_loss: 1.5918 - val_binary_accuracy: 0.8949 - val_precision_98: 0.8849 - val_recall_98: 0.9077 - val_false_negatives_98: 100.0000 - val_false_positives_98: 128.0000\n",
            "Epoch 20/20\n",
            "272/272 [==============================] - 1s 3ms/step - loss: 1.7571 - binary_accuracy: 0.8846 - precision_98: 0.8671 - recall_98: 0.9085 - false_negatives_98: 397.0000 - false_positives_98: 604.0000 - val_loss: 1.5654 - val_binary_accuracy: 0.8972 - val_precision_98: 0.9128 - val_recall_98: 0.8782 - val_false_negatives_98: 132.0000 - val_false_positives_98: 91.0000\n",
            "56/56 [==============================] - 0s 2ms/step\n",
            "Entrenamiento 1 (epochs=20, batch_size=32, validation_split=0.2):\n",
            "Metricas:\n",
            "Accuracy: 0.8941\n",
            "Precision (Class 1): 0.9026\n",
            "Recall (Class 1): 0.8834\n",
            "F1-score (Class 1): 0.8929\n",
            "Precision (Class 0): 0.8858\n",
            "Recall (Class 0): 0.9047\n",
            "F1-score (Class 0): 0.8952\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los hiperparametros fueron tomados de acuerdo a lo observado en los anteriores ejercicios realizados, podemos darnos cuenta de cuales valores de los hiperparametros nos dan mejores resultados, pero también se añadio lo visto en clase y en las ayudantías, ya que se eligieron estos valores por las siguientes razones:\n",
        "\n",
        "- Épocas de entrenamiento (epochs): Un número mayor de épocas de entrenamiento puede permitir que el modelo tenga más tiempo para aprender patrones complejos en los datos y mejorar su rendimiento. Sin embargo, usar un número excesivo de épocas puede provocar sobreajuste, donde el modelo se adapta demasiado a los datos de entrenamiento y no generaliza bien a datos nuevos. 20 épocas pueden ser suficientes para permitir que el modelo converja a una solución aceptable sin correr el riesgo de sobreajuste.\n",
        "\n",
        "- Tamaño del lote (batch size): El tamaño del lote se refiere al número de muestras de entrenamiento que se utilizan en una iteración durante el proceso de entrenamiento. Un tamaño de lote más pequeño puede ayudar a que el modelo converja más rápidamente y puede ser beneficioso para mejorar la generalización del modelo. Sin embargo, tamaños de lote demasiado pequeños pueden resultar en una convergencia más lenta del modelo y un tiempo de entrenamiento más largo. Se elige un tamaño de lote de 32, lo que proporciona un buen equilibrio entre velocidad de entrenamiento y estabilidad. Un tamaño de lote más grande puede ayudar a que el modelo converja más rápidamente y mejore la generalización del modelo, mientras que aún manteniendo un buen equilibrio entre velocidad y estabilidad..\n",
        "\n",
        "- Proporción de validación (validation split): La proporción de validación se refiere a la fracción de datos de entrenamiento que se reservan para la validación del modelo durante el entrenamiento. Esto permite monitorear el rendimiento del modelo en datos que no se utilizan para el entrenamiento y detectar signos de sobreajuste o subajuste. Una proporción de validación del 20% significa que el 20% de los datos de entrenamiento se usarán como conjunto de validación, lo que puede ser suficiente para evaluar el rendimiento del modelo sin reducir demasiado el tamaño del conjunto de entrenamiento.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zKb3N9JVV59s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "De acuerdo a los datos que salieron al momento, podemos ver que son bastante bueno ya que los resultados obtenidos son bastante buenos. Aquí hay una breve evaluación de cada una de las métricas:\n",
        "\n",
        "- Accuracy (Exactitud): Con un valor de 0.8941, significa que el 89.41% de las predicciones realizadas por el modelo son correctas. Este es un buen valor de accuracy y sugiere que el modelo está haciendo un buen trabajo en clasificar correctamente las muestras.\n",
        "\n",
        "- Precision (Precisión): Para la clase 1, la precisión es de 0.9026, lo que significa que el 90.26% de las predicciones positivas para la clase 1 son verdaderamente positivas. Para la clase 0, la precisión es de 0.8858, lo que significa que el 88.58% de las predicciones positivas para la clase 0 son verdaderamente positivas. Ambos valores de precisión son bastante altos, lo que indica que el modelo hace un buen trabajo en evitar falsos positivos.\n",
        "\n",
        "- Recall (Recuperación o Sensibilidad): Para la clase 1, el recall es de 0.8834, lo que significa que el 88.34% de los casos reales positivos fueron correctamente identificados por el modelo. Para la clase 0, el recall es de 0.9047, lo que significa que el 90.47% de los casos reales negativos fueron correctamente identificados por el modelo. Ambos valores de recall son altos, lo que indica que el modelo tiene una buena capacidad para identificar los casos positivos y negativos.\n",
        "\n",
        "- F1-score: El F1-score combina la precisión y el recall en una sola métrica, teniendo en cuenta tanto los falsos positivos como los falsos negativos. Con valores de F1-score de 0.8929 para la clase 1 y 0.8952 para la clase 0, estos indican un buen equilibrio entre precisión y recall para ambas clases.\n",
        "\n",
        "En resumen, los resultados obtenidos son buenos y sugieren que el modelo está funcionando bien en la tarea de clasificación."
      ],
      "metadata": {
        "id": "WIt_pFRfe78v"
      }
    }
  ]
}